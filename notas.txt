
 - mas grande sigma ==> en la matriz de similaridad la diagonal principal es mas grande
    - mas grande sigma ==> en la matriz de similaridad los valores fuera de la diagonal principal son mas peque침os

 - cuanto mas grande d ==> mas distintos los elementos (creemos que es porque cuanto mas grande hay mas dimensiones ==> la matriz es mas espec칤fica pero no es bueno eso nose xq)

 - PCA proyecta tus datos en un nuevo espacio de caracter칤sticas, donde las primeras componentes principales capturan la mayor parte de la variabilidad en tus datos. Las componentes principales subsiguientes capturan cada vez menos variabilidad.

Por lo tanto, si proyectas tus datos en un espacio de mayor dimensi칩n (es decir, eliges m치s componentes principales), es probable que veas m치s valores cercanos a cero en tu matriz. Esto se debe a que las componentes principales adicionales est치n capturando menos variabilidad en los datos, y por lo tanto, sus valores son m치s peque침os.

Esto es 칰til para la reducci칩n de la dimensionalidad, ya que puedes elegir mantener solo las primeras componentes principales (las que capturan la mayor parte de la variabilidad en tus datos) y descartar las restantes (las que tienen principalmente valores cercanos a cero).

 - esto no es tan seguro pero puede ser 칰til --> La raz칩n por la cual no observas una diferencia significativa en los valores de 
洧녲
k m치s all치 de 21 puede estar relacionada con la naturaleza de los datos y la descomposici칩n en valores singulares (SVD). Aqu칤 hay algunas razones por las que esto puede suceder:

Rango Efectivo de la Matriz 
洧녦
X:
La matriz 
洧녦
X tiene un rango que determina el n칰mero de valores singulares no nulos. Si el rango de 
洧녦
X es menor o igual a 21, entonces los valores singulares a partir de esa posici칩n ser치n cero o casi cero, lo que significa que no aportan nueva informaci칩n significativa. En otras palabras, m치s all치 de cierto punto, los componentes adicionales no contribuyen a reducir el error de reconstrucci칩n.

Varianza Explicada:
Los primeros componentes principales capturan la mayor parte de la varianza en los datos. Una vez que se ha capturado la mayor parte de la varianza significativa, los componentes adicionales contribuyen muy poco a la mejora de la reconstrucci칩n. Por lo tanto, la diferencia entre las im치genes originales y las reconstruidas se estabiliza.

Redundancia en los Datos:
Las im치genes pueden contener informaci칩n redundante, y una vez que los componentes principales han capturado las caracter칤sticas esenciales, agregar m치s componentes no mejora significativamente la reconstrucci칩n.

Para verificar esto, puedes calcular la varianza explicada acumulada y observar c칩mo cambia a medida que aumenta 
洧녲
k. Si la varianza explicada se estabiliza despu칠s de cierto punto, es una indicaci칩n de que los componentes adicionales no est치n aportando nueva informaci칩n.

