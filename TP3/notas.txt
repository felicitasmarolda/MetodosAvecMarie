
 - mas grande sigma ==> en la matriz de similaridad la diagonal principal es mas grande
    - mas grande sigma ==> en la matriz de similaridad los valores fuera de la diagonal principal 
    son mas peque침os

   - nos conviene que d sea 2 ya que tiene los mayores valores singulares

 - cuanto mas grande d ==> mas distintos los elementos (creemos que es porque cuanto mas grande 
 hay mas dimensiones ==> la matriz es mas espec칤fica pero no es bueno eso nose xq)

 - PCA proyecta tus datos en un nuevo espacio de caracter칤sticas, donde las primeras componentes 
 principales capturan la mayor parte de la variabilidad en tus datos. Las componentes principales 
 subsiguientes capturan cada vez menos variabilidad.

Por lo tanto, si proyectas tus datos en un espacio de mayor dimensi칩n (es decir, eliges m치s 
componentes principales), es probable que veas m치s valores cercanos a cero en tu matriz. 
Esto se debe a que las componentes principales adicionales est치n capturando menos variabilidad 
en los datos, y por lo tanto, sus valores son m치s peque침os.

Esto es 칰til para la reducci칩n de la dimensionalidad, ya que puedes elegir mantener solo las primeras 
componentes principales (las que capturan la mayor parte de la variabilidad en tus datos) y descartar 
las restantes (las que tienen principalmente valores cercanos a cero).
    -  al quedarse con pocas dimensiones (d칩nde las variables si son relevantes y uno elimina le 
    ruido) se observan clusters.
      -  al quedarse con muchas dimensiones (d칩nde las variables no son relevantes y uno no elimina 
      le ruido) se observan clusters. 
 - esto no es tan seguro pero puede ser 칰til --> La raz칩n por la cual no observas una diferencia 
 significativa en los valores de 
洧녲

k m치s all치 de 21 puede estar relacionada con la naturaleza de los datos y la descomposici칩n en 
valores singulares (SVD). Aqu칤 hay algunas razones por las que esto puede suceder:

Rango Efectivo de la Matriz 
洧녦
X:
La matriz 
洧녦
X tiene un rango que determina el n칰mero de valores singulares no nulos. Si el rango de 
洧녦
X es menor o igual a 21, entonces los valores singulares a partir de esa posici칩n ser치n cero o 
casi cero, lo que significa que no aportan nueva informaci칩n significativa. En otras palabras, 
m치s all치 de cierto punto, los componentes adicionales no contribuyen a reducir el error de 
reconstrucci칩n.

Varianza Explicada:
Los primeros componentes principales capturan la mayor parte de la varianza en los datos. Una vez que se ha capturado la mayor parte de la varianza significativa, los componentes adicionales contribuyen muy poco a la mejora de la reconstrucci칩n. Por lo tanto, la diferencia entre las im치genes originales y las reconstruidas se estabiliza.

Redundancia en los Datos:
Las im치genes pueden contener informaci칩n redundante, y una vez que los componentes principales han capturado las caracter칤sticas esenciales, agregar m치s componentes no mejora significativamente la reconstrucci칩n.

Para verificar esto, puedes calcular la varianza explicada acumulada y observar c칩mo cambia a medida que aumenta 
洧녲
k. Si la varianza explicada se estabiliza despu칠s de cierto punto, es una indicaci칩n de que los componentes adicionales no est치n aportando nueva informaci칩n.



Hay varias posibles explicaciones para este comportamiento aparentemente contradictorio:

Inestabilidad num칠rica: Los c치lculos de la descomposici칩n de valores singulares (SVD) y la pseudoinversa pueden ser sensibles a la presencia de valores singulares peque침os o cercanos a cero. Incluso peque침os errores de redondeo pueden causar inestabilidades cuando se trabaja con matrices mal condicionadas.
Multicolinealidad: Si las variables predictoras est치n altamente correlacionadas entre s칤 (multicolinealidad), agregar m치s dimensiones puede no mejorar significativamente el ajuste del modelo y podr칤a incluso empeorar el rendimiento debido al sobreajuste.
No linealidad: Si la relaci칩n subyacente entre las variables predictoras y la variable de respuesta no es lineal, un modelo lineal puede no ser capaz de capturar adecuadamente la estructura de los datos, independientemente del n칰mero de dimensiones.
Datos at칤picos o ruido: La presencia de datos at칤picos o ruido en los datos puede afectar la estimaci칩n de los coeficientes del modelo y, por lo tanto, el error de predicci칩n.

Para comprender mejor este comportamiento, puede ser 칰til realizar un an치lisis m치s detallado de los datos y el modelo. Algunas sugerencias:

Examinar los valores singulares y las condiciones de la matriz para detectar posibles problemas de inestabilidad num칠rica.
Calcular e inspeccionar las correlaciones entre las variables predictoras para detectar multicolinealidad.
Explorar transformaciones no lineales de las variables predictoras o utilizar modelos no lineales si se sospecha que la relaci칩n no es lineal.
Aplicar t칠cnicas de detecci칩n y tratamiento de datos at칤picos o realizar un proceso de limpieza de datos m치s exhaustivo.
Utilizar t칠cnicas de validaci칩n cruzada o conjuntos de datos de prueba separados para evaluar de manera m치s confiable el error de predicci칩n y evitar el sobreajuste.

En resumen, aunque es inusual, este comportamiento puede ocurrir debido a varios factores, y un an치lisis m치s profundo de los datos y el modelo puede ayudar a identificar la causa ra칤z y encontrar una soluci칩n adecuada.